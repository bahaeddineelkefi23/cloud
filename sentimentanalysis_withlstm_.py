# -*- coding: utf-8 -*-
"""SentimentAnalysis_withLSTM_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mf7VzeroVCi9yej4HtBGjvkvUbKkvwQ_

# ***Importation des packages***
"""

!pip install tensorflow-addons

# Import des bibliothèques nécessaires
import nltk # bibliothèque de traitement de langage naturel
import numpy as np # bibliothèque de calcul numérique en Python
import pandas as pd # bibliothèque pour manipuler des données tabulaires
from nltk.corpus import stopwords # module pour retirer les mots vides (stopwords) en anglais
from textblob import Word # bibliothèque de traitement de langage naturel pour le lemmatisation
from sklearn.preprocessing import LabelEncoder # module pour encoder les variables catégorielles en valeurs numériques
from collections import Counter # module pour compter les occurrences d'éléments dans une liste
import wordcloud # bibliothèque pour créer des nuages de mots
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score # fonctions pour évaluer les performances d'un modèle de classification
from tensorflow.keras.models import Sequential # bibliothèque pour créer des modèles de réseaux de neurones séquentiels
from tensorflow.keras.preprocessing.text import Tokenizer # classe pour transformer les textes en séquences de nombres
from tensorflow.keras.preprocessing.sequence import pad_sequences # classe pour effectuer un padding ou un troncage sur les séquences de nombres
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D # classes pour définir les couches d'un modèle de réseaux de neurones séquentiels
from sklearn.model_selection import train_test_split # fonction pour séparer les données en ensembles d'entraînement et de test
import matplotlib.pyplot as plt # bibliothèque pour visualiser des données en Python
from nltk.tokenize import word_tokenize # fonction pour diviser un texte en mots
from nltk.stem import WordNetLemmatizer # module pour lemmatiser les mots
from tensorflow.keras.callbacks import EarlyStopping # classe pour arrêter l'apprentissage prématurément dans un modèle de réseaux de neurones
import tensorflow_addons as tfa # bibliothèque d'add-ons pour TensorFlow
nltk.download('punkt')
nltk.download('wordnet')



"""# ***Importation de données***"""

# Lecture du fichier CSV "Tweets.csv" avec pandas et stockage des données dans l'objet DataFrame "df"
df = pd.read_csv("Tweets.csv")

Data = df[['non_punctuated_tweets','sentiment']]

"""# ***Exploration de données/Data Preprocessing***"""

Data

Data['tokenized_tweets'] = Data.apply(lambda row : nltk.word_tokenize(str(row['non_punctuated_tweets'])),axis = 1)  #tokenisation de tweets

Data

# Définition d'une fonction pour appliquer la lemmatisation à chaque mot dans la liste de mots tokenisés
def lemma(data):
    return " ".join([Word(word).lemmatize() for word in data])

# Application de la fonction de lemmatisation à la colonne "tokenized_tweets"
Data['lemmatized_tweets'] = Data['tokenized_tweets'].apply(lambda x: lemma(x))

Data

# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 500
# Max number of words in each tweets.
MAX_SEQUENCE_LENGTH = 50
# This is fixed.
EMBEDDING_DIM = 100

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, split=' ') 
tokenizer.fit_on_texts(Data.lemmatized_tweets.values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X = tokenizer.texts_to_sequences(Data.lemmatized_tweets.values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

Y = pd.get_dummies(Data.sentiment).values
print('Shape of label tensor:', Y.shape)

from numpy.random import seed
seed(1)
import tensorflow
tensorflow.random.set_seed(2)
f1 = tfa.metrics.F1Score(36,'micro' or 'macro')
model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',f1])

epochs = 7
batch_size = 64

Y

"""# ***Data preparation/Modeling LSTM***"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

history_1 = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

"""# ***Evaluation/testing***"""

accr = model.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

plt.figure(figsize=(8, 8))
plt.title('Loss')
plt.plot(history_1.history['loss'], label='train')
plt.plot(history_1.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.figure(figsize=(8, 8))
plt.title('Accuracy')
plt.plot(history_1.history['accuracy'], label='train')
plt.plot(history_1.history['val_accuracy'], label='test')
plt.legend()
plt.show();

tweet = ["I keep seeing all these posts about how  2021 was terrible for lots of people... and it's sad, because it was a very good year for me."]
seq = tokenizer.texts_to_sequences(tweet)
padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
pred = model.predict(padded)
labels = ['negative','neutral','positive']
print(pred, labels[np.argmax(pred)])

tweet = ["Im not ok i want to go to the cinema."]
seq = tokenizer.texts_to_sequences(tweet)
padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
pred = model.predict(padded)
labels = ['negative','neutral','positive']
print(pred, labels[np.argmax(pred)])